{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Smash that upvote button in you learned something new! "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tf-agents","execution_count":27,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: tf-agents in /opt/conda/lib/python3.7/site-packages (0.7.1)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from tf-agents) (1.19.5)\nRequirement already satisfied: cloudpickle>=1.3 in /opt/conda/lib/python3.7/site-packages (from tf-agents) (1.6.0)\nRequirement already satisfied: gin-config>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from tf-agents) (0.4.0)\nRequirement already satisfied: pillow>=7.0.0 in /opt/conda/lib/python3.7/site-packages (from tf-agents) (7.2.0)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tf-agents) (1.12.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from tf-agents) (3.7.4.3)\nRequirement already satisfied: gym>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from tf-agents) (0.18.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tf-agents) (1.15.0)\nRequirement already satisfied: protobuf>=3.11.3 in /opt/conda/lib/python3.7/site-packages (from tf-agents) (3.14.0)\nRequirement already satisfied: tensorflow-probability>=0.12.1 in /opt/conda/lib/python3.7/site-packages (from tf-agents) (0.12.1)\nRequirement already satisfied: absl-py>=0.6.1 in /opt/conda/lib/python3.7/site-packages (from tf-agents) (0.10.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym>=0.17.0->tf-agents) (1.4.1)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym>=0.17.0->tf-agents) (1.5.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.18.2)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from tensorflow-probability>=0.12.1->tf-agents) (4.4.2)\nRequirement already satisfied: gast>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.3.3)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.7/site-packages (from tensorflow-probability>=0.12.1->tf-agents) (0.1.5)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"from abc import ABC\nfrom random import choice\nimport tensorflow as tf\nimport numpy as np\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import q_network\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\n\nfrom kaggle_environments import make\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import *\n\nimport base64\nimport imageio\nimport IPython\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\n","execution_count":28,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build the training env"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"tf.compat.v1.enable_v2_behavior()\n\n\ndef get_board(env):\n    config = env.configuration\n    columns = config.columns\n    rows = config.rows\n\n    numeric_board = np.full([columns * rows], 10, dtype=int)\n\n    food_number = 5\n\n    for pos in env.state[0].observation.food:\n        numeric_board[pos] = food_number\n\n    for index, goose in enumerate(env.state[0].observation.geese):\n        for position in goose:\n            numeric_board[position] = index\n\n    #numeric_board = numeric_board.reshape((columns, rows))\n\n    return numeric_board\n\n\nclass GeeseEnv(py_environment.PyEnvironment):\n\n    def __init__(self):\n\n        self._env = make(\"hungry_geese\")\n        # The number of agents\n        self._NUM_AGENTS = 2\n\n        # Reset environment\n        observations = self._env.reset(num_agents=self._NUM_AGENTS)\n\n        self._state = get_board(self._env)\n\n        self._action_spec = array_spec.BoundedArraySpec(\n            shape=(), dtype=np.int32, minimum=0, maximum=3, name='action')\n        self._observation_spec = array_spec.BoundedArraySpec(\n            shape=(1, 1, self._state.shape[0]), dtype=np.int32, minimum=0, maximum=10,\n            name='observation')\n        self._episode_ended = False\n\n    def action_spec(self):\n        return self._action_spec\n\n    def observation_spec(self):\n        return self._observation_spec\n\n    def _reset(self):\n        observations = self._env.reset(num_agents=self._NUM_AGENTS)\n        self._state = [[get_board(self._env)]]\n        self._episode_ended = False\n        return ts.restart(np.array(self._state, dtype=np.int32))\n\n    def _step(self, action):\n\n        if self._episode_ended:\n            # The last action ended the episode. Ignore the current action and start\n            # a new episode.\n            return self.reset()\n\n        self._state = get_board(self._env)\n\n        choices = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n\n        actions = [choices[action], choice(choices)]\n\n        self._env.step(actions)\n\n        reward = self._env.steps[len(self._env.steps) - 1][0].reward\n\n        if self._env.done:\n            self._episode_ended = True\n\n        if self._episode_ended:\n            return ts.termination(np.array([[self._state]], dtype=np.int32), reward)\n        else:\n            return ts.transition(\n                np.array([[self._state]], dtype=np.int32), reward=reward, discount=1.0)\n","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = GeeseEnv()\nprint('action_spec:', env.action_spec())\nprint('time_step_spec.observation:', env.time_step_spec().observation)\nprint('time_step_spec.step_type:', env.time_step_spec().step_type)\nprint('time_step_spec.discount:', env.time_step_spec().discount)\nprint('time_step_spec.reward:', env.time_step_spec().reward)","execution_count":30,"outputs":[{"output_type":"stream","text":"action_spec: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=3)\ntime_step_spec.observation: BoundedArraySpec(shape=(1, 1, 77), dtype=dtype('int32'), name='observation', minimum=0, maximum=10)\ntime_step_spec.step_type: ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\ntime_step_spec.discount: BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\ntime_step_spec.reward: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Build the Bot\nFeel free to play with the hyper parms. All this code came from [here](https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial)"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_iterations = 10000  # @param {type:\"integer\"}\n\ninitial_collect_steps = 100  # @param {type:\"integer\"}\ncollect_steps_per_iteration = 1  # @param {type:\"integer\"}\nreplay_buffer_max_length = 100000  # @param {type:\"integer\"}\n\nbatch_size = 64  # @param {type:\"integer\"}\nlearning_rate = 1e-3  # @param {type:\"number\"}\nlog_interval = 200  # @param {type:\"integer\"}\n\nnum_eval_episodes = 10  # @param {type:\"integer\"}\neval_interval = 1000  # @param {type:\"integer\"}","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntf.compat.v1.enable_v2_behavior()\n\ntrain_py_env = GeeseEnv()\neval_py_env = GeeseEnv()\n\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\nfc_layer_params = (1000,)\n\nq_net = q_network.QNetwork(\n    train_env.observation_spec(),\n    train_env.action_spec(),\n    fc_layer_params=fc_layer_params)\n\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n\ntrain_step_counter = tf.Variable(0)\n\nagent = dqn_agent.DqnAgent(\n    train_env.time_step_spec(),\n    train_env.action_spec(),\n    q_network=q_net,\n    optimizer=optimizer,\n    td_errors_loss_fn=common.element_wise_squared_loss,\n    train_step_counter=train_step_counter)\n\nagent.initialize()\n\neval_policy = agent.policy\ncollect_policy = agent.collect_policy\n\nrandom_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n                                                train_env.action_spec())\n\n\ndef compute_avg_return(environment, policy, num_episodes=10):\n    total_return = 0.0\n    for _ in range(num_episodes):\n\n        time_step = environment.reset()\n        episode_return = 0.0\n\n        while not time_step.is_last():\n            action_step = policy.action(time_step)\n            time_step = environment.step(action_step.action)\n            episode_return += time_step.reward\n        total_return += episode_return\n\n    avg_return = total_return / num_episodes\n    return avg_return.numpy()[0]\n\n\ncompute_avg_return(eval_env, random_policy, num_eval_episodes)\n\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n    data_spec=agent.collect_data_spec,\n    batch_size=train_env.batch_size,\n    max_length=replay_buffer_max_length)\n\n\ndef collect_step(environment, policy, buffer):\n    time_step = environment.current_time_step()\n    action_step = policy.action(time_step)\n    next_time_step = environment.step(action_step.action)\n    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n\n    # Add trajectory to the replay buffer\n    buffer.add_batch(traj)\n\n\ndef collect_data(env, policy, buffer, steps):\n    for _ in range(steps):\n        collect_step(env, policy, buffer)\n\n\ncollect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n\n# This loop is so common in RL, that we provide standard implementations.\n# For more details see the drivers module.\n# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers\n\n\ndataset = replay_buffer.as_dataset(\n    num_parallel_calls=3,\n    sample_batch_size=batch_size,\n    num_steps=2).prefetch(3)\n\niterator = iter(dataset)\n\nprint(iterator)\n\n# (Optional) Optimize by wrapping some of the code in a graph using TF function.\nagent.train = common.function(agent.train)\n\n# Reset the train step\nagent.train_step_counter.assign(0)\n\n# Evaluate the agent's policy once before training.\navg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\nreturns = [avg_return]\n\ntf_policy_saver = policy_saver.PolicySaver(agent.policy)\n\nfor _ in range(num_iterations):\n\n    # Collect a few steps using collect_policy and save to the replay buffer.\n    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n\n    # Sample a batch of data from the buffer and update the agent's network.\n    experience, unused_info = next(iterator)\n    train_loss = agent.train(experience).loss\n\n    step = agent.train_step_counter.numpy()\n\n    if step % log_interval == 0:\n        print('step = {0}: loss = {1}'.format(step, train_loss))\n\n    if step % eval_interval == 0:\n        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n        returns.append(avg_return)\n        tf_policy_saver.save('current_policy')\n","execution_count":null,"outputs":[{"output_type":"stream","text":"<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f3220553250>\nstep = 200: loss = 2552054.0\nstep = 400: loss = 16882.166015625\nstep = 600: loss = 352363.53125\nstep = 800: loss = 98336.484375\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"saved_policy = tf.compat.v2.saved_model.load('current_policy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tf_agents.trajectories import time_step as ts\n\nblank_board = np.zeros([1,1,77], dtype=np.int32)\nprint(blank_board.shape)\nstep_type = tf.convert_to_tensor(\n    [0], dtype=tf.int32, name='step_type')\nreward = tf.convert_to_tensor(\n    [0], dtype=tf.float32, name='reward')\ndiscount = tf.convert_to_tensor(\n    [1], dtype=tf.float32, name='discount')\nobservations = tf.convert_to_tensor(\n    [blank_board], dtype=tf.int32, name='observations')\ntimestep = ts.TimeStep(step_type, reward, discount, observations)\n\ntime_step = None\naction_step = saved_policy.action(timestep)\nprint(action_step)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_board(ob, co):\n    config = co\n    columns = config.columns\n    rows = config.rows\n\n    numeric_board = np.full([columns * rows], 10, dtype=int)\n\n    food_number = 5\n\n    for pos in ob.food:\n        numeric_board[pos] = food_number\n\n    for index, goose in enumerate(ob.geese):\n        for position in goose:\n            numeric_board[position] = index\n\n    return numeric_board","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n\n\ndef agent(obs_dict, config_dict):\n    \"\"\"This agent always moves toward observation.food[0] but does not take advantage of board wrapping\"\"\"\n    this_board = np.array([[get_board(obs_dict, config_dict)]])\n    \n    step_type = tf.convert_to_tensor(\n        [0], dtype=tf.int32, name='step_type')\n    reward = tf.convert_to_tensor(\n        [0], dtype=tf.float32, name='reward')\n    discount = tf.convert_to_tensor(\n        [1], dtype=tf.float32, name='discount')\n    observations = tf.convert_to_tensor(\n        [this_board], dtype=tf.int32, name='observations')\n    timestep = ts.TimeStep(step_type, reward, discount, observations)\n\n    action = saved_policy.action(timestep)\n    \n    choices = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n    \n    choice = choices[int(action.action)]\n    \n    print(f\"choice:{choice}\")\n    return choice\n\n\nfrom kaggle_environments import evaluate, make, utils\n\n# Setup a hungry_geese environment.\nenv = make(\"hungry_geese\", debug = True)\nenv.run([agent, \"random\"])\nenv.render(mode=\"ipython\", width=600, height=650)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"","execution_count":80,"outputs":[{"output_type":"stream","text":"Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/kaggle_environments/agent.py\", line 151, in act\n    action = self.agent(*args)\n  File \"<ipython-input-79-2fbd9b39edd2>\", line 11, in agent\n    action = agent.policy.action(None)\nAttributeError: 'function' object has no attribute 'policy'\nError: ['Traceback (most recent call last):\\n', '  File \"/opt/conda/lib/python3.7/site-packages/kaggle_environments/agent.py\", line 151, in act\\n    action = self.agent(*args)\\n', '  File \"<ipython-input-79-2fbd9b39edd2>\", line 11, in agent\\n    action = agent.policy.action(None)\\n', \"AttributeError: 'function' object has no attribute 'policy'\\n\"]\n","name":"stdout"},{"output_type":"execute_result","execution_count":80,"data":{"text/plain":"[[{'action': 'NORTH',\n   'reward': 0,\n   'info': {},\n   'observation': {'remainingOverageTime': 12,\n    'step': 0,\n    'geese': [[1], [14]],\n    'food': [62, 3],\n    'index': 0},\n   'status': 'ACTIVE'},\n  {'action': 'NORTH',\n   'reward': 0,\n   'info': {},\n   'observation': {'remainingOverageTime': 12, 'index': 1},\n   'status': 'ACTIVE'}],\n [{'action': None,\n   'reward': None,\n   'info': {},\n   'observation': {'remainingOverageTime': 12,\n    'step': 1,\n    'geese': [[1], [13]],\n    'food': [62, 3],\n    'index': 0},\n   'status': 'ERROR'},\n  {'action': 'WEST',\n   'reward': 2,\n   'info': {},\n   'observation': {'remainingOverageTime': 12, 'index': 1},\n   'status': 'DONE'}]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}